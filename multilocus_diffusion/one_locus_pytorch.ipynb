{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "[7980 6947  139 ... 2881 2459 1025]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_train = 5000\n",
    "# Load Data # replace with something else (also will make .pkl file)\n",
    "data = np.load('single_locus_data.npy', allow_pickle='TRUE')\n",
    "# with open(\"single_locus_data.pkl\", \"wb\") as tf:\n",
    "#     data = pickle.load(tf)    \n",
    "f = data.item()['f'][:, None]\n",
    "t = data.item()['t'][:, None]\n",
    "p = data.item()['phi'][:, None]\n",
    "print(len(f))\n",
    "idx_list = np.arange(len(f))\n",
    "np.random.shuffle(idx_list)\n",
    "print(idx_list)\n",
    "\n",
    "f_train = torch.from_numpy(f[idx_list[:N_train]]).float()\n",
    "t_train = torch.from_numpy(t[idx_list[:N_train]]).float()\n",
    "p_train = torch.from_numpy(p[idx_list[:N_train]]).float()\n",
    "f_test = torch.from_numpy(f[idx_list[N_train:]]).float()\n",
    "t_test = torch.from_numpy(t[idx_list[N_train:]]).float()\n",
    "p_test = torch.from_numpy(p[idx_list[N_train:]]).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers, s0, N0):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(layers)-3):\n",
    "            self.model.add_module(f\"layer{i}\", nn.Linear(layers[i], layers[i+1]))\n",
    "            self.model.add_module(f\"activation{i}\", nn.Tanh())\n",
    "        self.model.add_module(f\"layer{len(layers)-2}\", nn.Linear(layers[-3], layers[-2]))\n",
    "        self.model.add_module(f\"activation{len(layers)-2}\", nn.Softplus())\n",
    "        self.model.add_module(f\"layer{len(layers)-1}\", nn.Linear(layers[-2], layers[-1]))\n",
    "        self.s = Variable(torch.tensor(s0)).to(device)\n",
    "        self.N = Variable(torch.tensor(N0)).to(device)\n",
    "\n",
    "    def forward(self, f, t):\n",
    "        inputs = torch.cat([f, t], 1)\n",
    "        return self.model(inputs)\n",
    "    def PDE(self, f, t):\n",
    "        s = self.s\n",
    "        N = self.N\n",
    "        p = self.model(torch.cat([f, t], 1))\n",
    "        p_f = torch.autograd.grad(p.sum(), f, create_graph=True)[0]\n",
    "        p_t = torch.autograd.grad(p.sum(), t, create_graph=True)[0]\n",
    "        p_ff = torch.autograd.grad(p_f.sum(), f, create_graph=True)[0]\n",
    "        g = p_t + s * ((1 - 2*f)*p + (f - f**2)*p_f) + 1 / (2*N) * (-2*p + 2*(1 - 2*f)*p_f + (f - f**2)*p_ff)\n",
    "        return g\n",
    "\n",
    "# Input = (f, t) series, Output = p series\n",
    "model = PINN(layers = [2, 20, 20, 20, 1], s0 = 0.1, N0 = 500).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN(\n",
      "  (model): Sequential(\n",
      "    (layer0): Linear(in_features=2, out_features=20, bias=True)\n",
      "    (activation0): Tanh()\n",
      "    (layer1): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (activation1): Tanh()\n",
      "    (layer3): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (activation3): Softplus(beta=1, threshold=20)\n",
      "    (layer4): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1659],\n",
       "        [0.1659],\n",
       "        [0.1642],\n",
       "        ...,\n",
       "        [0.1659],\n",
       "        [0.1659],\n",
       "        [0.1659]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(f_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now define loss based on a PDE. We need to get derivative of p against x and t.\n",
    "MSE_loss = nn.MSELoss()\n",
    "f_sample = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "pt_f_sample = Variable(torch.from_numpy(f_sample).float(), requires_grad=True).to(device)\n",
    "t_sample = np.random.uniform(low=0.0, high=10000, size=(500,1))\n",
    "pt_t_sample = Variable(torch.from_numpy(t_sample).float(), requires_grad=True).to(device)\n",
    "loss_pde = MSE_loss(model.PDE(pt_f_sample, pt_t_sample), torch.zeros_like(model.PDE(pt_f_sample, pt_t_sample)))\n",
    "# In this case, we have p_train to compare directly against estimate of p at f_train, t_train. \n",
    "loss_data = MSE_loss(model(f_train, t_train), p_train)\n",
    "loss = loss_pde + loss_data\n",
    "\n",
    "# In a different scenario, I will put in all f, t data without estimating p before training. \n",
    "# reorganize f and t series so that I have f at each time point (t_i), add - 1/n_i sum(log(p(f(t_i), t_i))) + int model(f, t_i)df\n",
    "# sort the data by time point, start from t=0 till maximum t (either t_extinct or t_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 training loss: tensor(0.3438)\n",
      "1 training loss: tensor(0.3079)\n",
      "2 training loss: tensor(0.2741)\n",
      "3 training loss: tensor(0.2418)\n",
      "4 training loss: tensor(0.2101)\n",
      "5 training loss: tensor(0.1777)\n",
      "6 training loss: tensor(0.1528)\n",
      "7 training loss: tensor(0.1326)\n",
      "8 training loss: tensor(0.1147)\n",
      "9 training loss: tensor(0.0990)\n",
      "10 training loss: tensor(0.0872)\n",
      "11 training loss: tensor(0.0777)\n",
      "12 training loss: tensor(0.0700)\n",
      "13 training loss: tensor(0.0638)\n",
      "14 training loss: tensor(0.0591)\n",
      "15 training loss: tensor(0.0557)\n",
      "16 training loss: tensor(0.0536)\n",
      "17 training loss: tensor(0.0524)\n",
      "18 training loss: tensor(0.0521)\n",
      "19 training loss: tensor(0.0525)\n",
      "20 training loss: tensor(0.0533)\n",
      "21 training loss: tensor(0.0545)\n",
      "22 training loss: tensor(0.0557)\n",
      "23 training loss: tensor(0.0569)\n",
      "24 training loss: tensor(0.0579)\n",
      "25 training loss: tensor(0.0586)\n",
      "26 training loss: tensor(0.0590)\n",
      "27 training loss: tensor(0.0591)\n",
      "28 training loss: tensor(0.0588)\n",
      "29 training loss: tensor(0.0582)\n",
      "30 training loss: tensor(0.0573)\n",
      "31 training loss: tensor(0.0562)\n",
      "32 training loss: tensor(0.0551)\n",
      "33 training loss: tensor(0.0539)\n",
      "34 training loss: tensor(0.0526)\n",
      "35 training loss: tensor(0.0527)\n",
      "36 training loss: tensor(0.0524)\n",
      "37 training loss: tensor(0.0513)\n",
      "38 training loss: tensor(0.0506)\n",
      "39 training loss: tensor(0.0504)\n",
      "40 training loss: tensor(0.0501)\n",
      "41 training loss: tensor(0.0499)\n",
      "42 training loss: tensor(0.0497)\n",
      "43 training loss: tensor(0.0498)\n",
      "44 training loss: tensor(0.0499)\n",
      "45 training loss: tensor(0.0497)\n",
      "46 training loss: tensor(0.0497)\n",
      "47 training loss: tensor(0.0496)\n",
      "48 training loss: tensor(0.0495)\n",
      "49 training loss: tensor(0.0494)\n",
      "50 training loss: tensor(0.0493)\n",
      "51 training loss: tensor(0.0492)\n",
      "52 training loss: tensor(0.0490)\n",
      "53 training loss: tensor(0.0489)\n",
      "54 training loss: tensor(0.0486)\n",
      "55 training loss: tensor(0.0483)\n",
      "56 training loss: tensor(0.0479)\n",
      "57 training loss: tensor(0.0475)\n",
      "58 training loss: tensor(0.0473)\n",
      "59 training loss: tensor(0.0475)\n",
      "60 training loss: tensor(0.0473)\n",
      "61 training loss: tensor(0.0469)\n",
      "62 training loss: tensor(0.0467)\n",
      "63 training loss: tensor(0.0466)\n",
      "64 training loss: tensor(0.0464)\n",
      "65 training loss: tensor(0.0464)\n",
      "66 training loss: tensor(0.0464)\n",
      "67 training loss: tensor(0.0463)\n",
      "68 training loss: tensor(0.0461)\n",
      "69 training loss: tensor(0.0460)\n",
      "70 training loss: tensor(0.0458)\n",
      "71 training loss: tensor(0.0457)\n",
      "72 training loss: tensor(0.0455)\n",
      "73 training loss: tensor(0.0454)\n",
      "74 training loss: tensor(0.0454)\n",
      "75 training loss: tensor(0.0453)\n",
      "76 training loss: tensor(0.0453)\n",
      "77 training loss: tensor(0.0451)\n",
      "78 training loss: tensor(0.0450)\n",
      "79 training loss: tensor(0.0449)\n",
      "80 training loss: tensor(0.0448)\n",
      "81 training loss: tensor(0.0447)\n",
      "82 training loss: tensor(0.0446)\n",
      "83 training loss: tensor(0.0445)\n",
      "84 training loss: tensor(0.0444)\n",
      "85 training loss: tensor(0.0443)\n",
      "86 training loss: tensor(0.0442)\n",
      "87 training loss: tensor(0.0441)\n",
      "88 training loss: tensor(0.0440)\n",
      "89 training loss: tensor(0.0439)\n",
      "90 training loss: tensor(0.0438)\n",
      "91 training loss: tensor(0.0437)\n",
      "92 training loss: tensor(0.0436)\n",
      "93 training loss: tensor(0.0435)\n",
      "94 training loss: tensor(0.0434)\n",
      "95 training loss: tensor(0.0433)\n",
      "96 training loss: tensor(0.0432)\n",
      "97 training loss: tensor(0.0431)\n",
      "98 training loss: tensor(0.0430)\n",
      "99 training loss: tensor(0.0429)\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "    MSE_loss = nn.MSELoss()\n",
    "    f_sample = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "    pt_f_sample = Variable(torch.from_numpy(f_sample).float(), requires_grad=True).to(device)\n",
    "    t_sample = np.random.uniform(low=0.0, high=10000, size=(500,1))\n",
    "    pt_t_sample = Variable(torch.from_numpy(t_sample).float(), requires_grad=True).to(device)\n",
    "    loss_pde = MSE_loss(model.PDE(pt_f_sample, pt_t_sample), torch.zeros_like(model.PDE(pt_f_sample, pt_t_sample)))\n",
    "    # In this case, we have p_train to compare directly against estimate of p at f_train, t_train. \n",
    "    loss_data = MSE_loss(model(f_train, t_train), p_train)\n",
    "    loss = loss_pde + loss_data\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        print(epoch,\"training loss:\", loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return predicted s and N, plot p(f, t). Plot p(f = 1, t), which is fixation probability as a function of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
