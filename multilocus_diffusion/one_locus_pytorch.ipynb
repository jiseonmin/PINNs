{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "[ 5958  7800  8340 ...  6742  5191 10164]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_train = 5000\n",
    "# Load Data # replace with something else (also will make .pkl file)\n",
    "data = np.load('single_locus_data.npy', allow_pickle='TRUE')\n",
    "# with open(\"single_locus_data.pkl\", \"wb\") as tf:\n",
    "#     data = pickle.load(tf)    \n",
    "f = data.item()['f'][:, None]\n",
    "t = data.item()['t'][:, None]\n",
    "p = data.item()['phi'][:, None]\n",
    "print(len(f))\n",
    "idx_list = np.arange(len(f))\n",
    "np.random.shuffle(idx_list)\n",
    "print(idx_list)\n",
    "\n",
    "f_train = torch.from_numpy(f[idx_list[:N_train]]).float()\n",
    "t_train = torch.from_numpy(t[idx_list[:N_train]]).float()\n",
    "p_train = torch.from_numpy(p[idx_list[:N_train]]).float()\n",
    "f_test = torch.from_numpy(f[idx_list[N_train:]]).float()\n",
    "t_test = torch.from_numpy(t[idx_list[N_train:]]).float()\n",
    "p_test = torch.from_numpy(p[idx_list[N_train:]]).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers, s0, N0):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(layers)-3):\n",
    "            self.model.add_module(f\"layer{i}\", nn.Linear(layers[i], layers[i+1]))\n",
    "            self.model.add_module(f\"activation{i}\", nn.Tanh())\n",
    "        self.model.add_module(f\"layer{len(layers)-2}\", nn.Linear(layers[-3], layers[-2]))\n",
    "        self.model.add_module(f\"activation{len(layers)-2}\", nn.Softplus())\n",
    "        self.model.add_module(f\"layer{len(layers)-1}\", nn.Linear(layers[-2], layers[-1]))\n",
    "        self.s = torch.nn.Parameter(torch.tensor(s0).float(), requires_grad = True)\n",
    "        self.N = torch.nn.Parameter(torch.tensor(N0).float(), requires_grad = True)\n",
    "        # self.s = Variable(torch.tensor(s0).float(), requires_grad = True).to(device)\n",
    "        # self.N = Variable(torch.tensor(N0).float(), requires_grad = True).to(device)\n",
    "\n",
    "    def forward(self, f, t):\n",
    "        inputs = torch.cat([f, t], 1)\n",
    "        return self.model(inputs)\n",
    "    def PDE(self, f, t):\n",
    "        s = self.s\n",
    "        N = self.N\n",
    "        p = self.model(torch.cat([f, t], 1))\n",
    "        p_f = torch.autograd.grad(p.sum(), f, create_graph=True)[0]\n",
    "        p_t = torch.autograd.grad(p.sum(), t, create_graph=True)[0]\n",
    "        p_ff = torch.autograd.grad(p_f.sum(), f, create_graph=True)[0]\n",
    "        g = p_t + s * ((1 - 2*f)*p + (f - f**2)*p_f) + 1 / (2*N) * (-2*p + 2*(1 - 2*f)*p_f + (f - f**2)*p_ff)\n",
    "        return g\n",
    "\n",
    "# Input = (f, t) series, Output = p series\n",
    "model = PINN(layers = [2, 20, 20, 20, 1], s0 = 0.1, N0 = 500).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN(\n",
      "  (model): Sequential(\n",
      "    (layer0): Linear(in_features=2, out_features=20, bias=True)\n",
      "    (activation0): Tanh()\n",
      "    (layer1): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (activation1): Tanh()\n",
      "    (layer3): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (activation3): Softplus(beta=1, threshold=20)\n",
      "    (layer4): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor(0.1000, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(500., requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2094, -0.5100],\n",
      "        [-0.3868, -0.5020],\n",
      "        [-0.2009,  0.1741],\n",
      "        [ 0.3426, -0.6366],\n",
      "        [ 0.3686, -0.5230],\n",
      "        [ 0.0813,  0.6961],\n",
      "        [-0.4961, -0.6420],\n",
      "        [ 0.6010,  0.2583],\n",
      "        [ 0.3230, -0.7067],\n",
      "        [ 0.1903, -0.0757],\n",
      "        [ 0.6649, -0.2768],\n",
      "        [ 0.1049,  0.2885],\n",
      "        [-0.6545,  0.3637],\n",
      "        [ 0.4484, -0.5865],\n",
      "        [ 0.0009,  0.5008],\n",
      "        [-0.2942, -0.1481],\n",
      "        [-0.5370,  0.2414],\n",
      "        [ 0.4745,  0.5884],\n",
      "        [-0.3086, -0.2280],\n",
      "        [-0.3501, -0.5310]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6016, -0.2704,  0.0880, -0.0687,  0.0722,  0.6329, -0.4123, -0.2125,\n",
      "        -0.1013, -0.3996,  0.6824,  0.2551, -0.2323, -0.2132, -0.6971,  0.2753,\n",
      "         0.0754, -0.1760,  0.5529,  0.4442], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1497,  0.0117,  0.0367, -0.0505,  0.1548, -0.0021, -0.0487,  0.1897,\n",
      "          0.0577, -0.1636,  0.1135, -0.0626,  0.0545,  0.2176, -0.1202,  0.1361,\n",
      "          0.0091,  0.2025, -0.2126, -0.0312],\n",
      "        [ 0.0995, -0.0737, -0.1590,  0.0249, -0.1474,  0.2125,  0.0539, -0.1872,\n",
      "          0.0667, -0.1993, -0.0640, -0.2178, -0.1780,  0.0135, -0.0489,  0.2168,\n",
      "          0.1261,  0.0626, -0.0181, -0.1690],\n",
      "        [-0.1353, -0.0585,  0.1535,  0.1362,  0.1282, -0.1261,  0.0286,  0.0991,\n",
      "         -0.0228, -0.0509, -0.2007,  0.1203,  0.1250, -0.1075, -0.0385, -0.0895,\n",
      "         -0.0065, -0.0716, -0.1701, -0.0011],\n",
      "        [ 0.2008, -0.1414, -0.1207, -0.0753,  0.1246,  0.1921, -0.0665,  0.1204,\n",
      "          0.2041, -0.0814,  0.0574, -0.0763,  0.0055,  0.1348,  0.1986, -0.0966,\n",
      "          0.0357, -0.0779,  0.1966, -0.0849],\n",
      "        [ 0.2088, -0.0688,  0.1406, -0.2093, -0.2043,  0.1484, -0.0496,  0.2049,\n",
      "          0.1427,  0.2141, -0.0838,  0.1378,  0.1267, -0.0752,  0.0531,  0.0388,\n",
      "         -0.1114, -0.1187,  0.0395, -0.0322],\n",
      "        [-0.1516, -0.0375, -0.0614, -0.0769, -0.1147,  0.0723,  0.1471,  0.1164,\n",
      "          0.1706,  0.0847, -0.0888, -0.1568,  0.0531, -0.1274,  0.0207, -0.0314,\n",
      "         -0.2153, -0.0758, -0.1774,  0.0933],\n",
      "        [ 0.0818,  0.1065,  0.1017, -0.1044, -0.0856, -0.2080,  0.0469, -0.2083,\n",
      "         -0.0460, -0.0152,  0.0515, -0.0427, -0.1619,  0.0672,  0.0552, -0.1749,\n",
      "         -0.0977,  0.1571,  0.1129,  0.0191],\n",
      "        [-0.1989, -0.2105, -0.0509, -0.1115,  0.1354, -0.1086,  0.1526,  0.1703,\n",
      "          0.1104, -0.1284,  0.1087, -0.0298, -0.0691, -0.1397, -0.0710,  0.1817,\n",
      "         -0.0161,  0.0200, -0.2140,  0.1677],\n",
      "        [-0.0214,  0.1660, -0.0124,  0.0887, -0.0645, -0.1058,  0.0046,  0.2228,\n",
      "          0.2068,  0.1839,  0.0446, -0.1635, -0.1559, -0.0594,  0.1009,  0.0204,\n",
      "         -0.0562,  0.0553, -0.2168,  0.0692],\n",
      "        [ 0.0639,  0.1786,  0.0601,  0.0025,  0.1358,  0.1619, -0.1551,  0.1270,\n",
      "          0.1927, -0.2186,  0.1733,  0.0779, -0.1663,  0.1590, -0.1838, -0.1102,\n",
      "          0.2228, -0.1375,  0.1853, -0.0940],\n",
      "        [ 0.0542, -0.0681,  0.2111,  0.1813, -0.0216,  0.0107,  0.0858, -0.1018,\n",
      "         -0.1102,  0.1782,  0.0051, -0.1316, -0.1502,  0.1120,  0.1652, -0.0441,\n",
      "         -0.0650, -0.0509,  0.1290,  0.0418],\n",
      "        [ 0.0874, -0.1025,  0.0613,  0.1166,  0.1885,  0.1747, -0.2195,  0.1514,\n",
      "         -0.1851,  0.1358,  0.0823, -0.2182, -0.1711,  0.1370, -0.1319,  0.1786,\n",
      "          0.0785, -0.1407,  0.0588,  0.0008],\n",
      "        [-0.0129,  0.2074,  0.0858, -0.0514,  0.1323,  0.0946, -0.0073, -0.1270,\n",
      "          0.1925, -0.0112, -0.1541, -0.0885,  0.1475,  0.1989, -0.0890, -0.0012,\n",
      "         -0.1118,  0.0005, -0.0549, -0.2120],\n",
      "        [-0.0085,  0.1588,  0.1216, -0.1315, -0.1651, -0.0522,  0.1595, -0.0974,\n",
      "          0.1361,  0.0049,  0.0322, -0.0229,  0.0829,  0.2206,  0.1273, -0.1979,\n",
      "         -0.1131, -0.0639,  0.1925, -0.0493],\n",
      "        [ 0.0487,  0.1255, -0.1740, -0.0184, -0.0882, -0.0516,  0.1414, -0.1687,\n",
      "          0.0863, -0.0020,  0.0602, -0.1415,  0.2160,  0.1184,  0.1164, -0.1052,\n",
      "          0.1447, -0.1769, -0.1195,  0.2034],\n",
      "        [-0.1013,  0.0720,  0.1699, -0.0412,  0.1583,  0.1703,  0.1104, -0.1600,\n",
      "         -0.0668, -0.1016, -0.2072,  0.1857, -0.1612,  0.1223,  0.0564, -0.0779,\n",
      "          0.1481, -0.1091, -0.1727,  0.1493],\n",
      "        [-0.0832,  0.1391,  0.1630,  0.0710, -0.0380,  0.0333, -0.0678,  0.1850,\n",
      "          0.0632, -0.1853, -0.0284,  0.1198,  0.1652, -0.1336, -0.2211,  0.1733,\n",
      "         -0.1752,  0.0823, -0.0828, -0.1952],\n",
      "        [ 0.1383, -0.1364,  0.1298, -0.0548,  0.1444,  0.1397,  0.1724,  0.1658,\n",
      "          0.1991,  0.1835, -0.0114, -0.0691,  0.2234,  0.1658, -0.1460, -0.0810,\n",
      "          0.1932, -0.0623,  0.1910, -0.0287],\n",
      "        [-0.1507, -0.0092, -0.0016,  0.1040,  0.0265, -0.1720, -0.0638, -0.0277,\n",
      "          0.0595, -0.0010, -0.0577, -0.1212, -0.1068,  0.1255,  0.1589, -0.1980,\n",
      "          0.1400, -0.0110,  0.1052, -0.0065],\n",
      "        [-0.1255, -0.0163,  0.1157, -0.0855, -0.1410, -0.0302,  0.0558, -0.1904,\n",
      "          0.1973,  0.2112, -0.0372, -0.1835, -0.0744,  0.1427, -0.0989,  0.0833,\n",
      "         -0.1401,  0.1866,  0.0243,  0.1288]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0436,  0.0920,  0.1368,  0.0124,  0.1860, -0.2118,  0.0547, -0.1373,\n",
      "        -0.1258, -0.0287, -0.1183,  0.1045,  0.1504, -0.0495,  0.1798,  0.0786,\n",
      "        -0.1490,  0.2078, -0.1993, -0.1751], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0053,  0.0087, -0.0935, -0.0815, -0.1459, -0.0868,  0.1627, -0.0784,\n",
      "         -0.1720,  0.1779,  0.1126, -0.0874,  0.0146,  0.2219, -0.0071, -0.0261,\n",
      "         -0.0549,  0.1940,  0.1655, -0.1890],\n",
      "        [-0.0873,  0.2082, -0.2065, -0.0322,  0.2136,  0.2028, -0.1350,  0.1342,\n",
      "          0.1309,  0.1232,  0.1823,  0.0484,  0.0446,  0.0250,  0.0572, -0.0919,\n",
      "          0.2195,  0.1893,  0.0613,  0.0393],\n",
      "        [-0.0353, -0.2053, -0.1517, -0.1996, -0.2114, -0.1453, -0.0155, -0.0993,\n",
      "          0.0549,  0.0134,  0.0800, -0.0384,  0.1968, -0.1093, -0.1571, -0.0246,\n",
      "          0.0989,  0.0404,  0.1526,  0.0168],\n",
      "        [ 0.2056, -0.1106, -0.0414,  0.0889,  0.0950,  0.0440,  0.1813,  0.1218,\n",
      "          0.1153, -0.2198,  0.0203,  0.1711,  0.1252,  0.0611,  0.2055, -0.0310,\n",
      "         -0.0299, -0.1644,  0.0544, -0.2009],\n",
      "        [ 0.0138, -0.0390, -0.0156, -0.0878, -0.1443,  0.1631, -0.0419, -0.1178,\n",
      "          0.1395,  0.1365,  0.1399, -0.0719,  0.1777,  0.0487, -0.0942, -0.0303,\n",
      "          0.1781,  0.0686,  0.1506, -0.1074],\n",
      "        [-0.1036, -0.0819,  0.2073, -0.1145, -0.1589,  0.1709,  0.0374,  0.1964,\n",
      "          0.0781,  0.1563,  0.0431, -0.1312, -0.0195,  0.1620, -0.1246,  0.0574,\n",
      "          0.1540,  0.2043,  0.0870, -0.0646],\n",
      "        [ 0.1564, -0.0775, -0.0447,  0.0821, -0.0964, -0.1164, -0.1430, -0.0447,\n",
      "         -0.0074, -0.0369, -0.0558, -0.0506, -0.0235, -0.0423,  0.0013, -0.0260,\n",
      "         -0.0518,  0.1805, -0.2133,  0.1286],\n",
      "        [-0.1248, -0.1232,  0.0776, -0.1698,  0.1833, -0.1792,  0.1038, -0.1574,\n",
      "          0.0749, -0.0719,  0.0971,  0.2055,  0.0123,  0.2094,  0.1180,  0.2201,\n",
      "          0.1016, -0.1638, -0.1202, -0.1866],\n",
      "        [ 0.2023,  0.0137, -0.1646, -0.0848,  0.0174, -0.1819, -0.0901, -0.0780,\n",
      "          0.0148, -0.0538, -0.2234,  0.0182,  0.0540, -0.1662,  0.1720, -0.1304,\n",
      "         -0.1763,  0.0411, -0.1809, -0.0877],\n",
      "        [ 0.0020, -0.0983,  0.1829,  0.1166,  0.0055, -0.1903,  0.1809, -0.0836,\n",
      "         -0.1747, -0.0145, -0.1485, -0.0838,  0.1134, -0.2026, -0.1554, -0.1981,\n",
      "          0.1443, -0.1922, -0.0974,  0.1127],\n",
      "        [-0.0080, -0.1897, -0.1789,  0.1360, -0.1983, -0.1393, -0.1736, -0.1065,\n",
      "         -0.0798,  0.0605, -0.2170, -0.1598, -0.1313, -0.1918, -0.1141, -0.0209,\n",
      "          0.1467, -0.1992, -0.0410,  0.0501],\n",
      "        [-0.0227, -0.2079, -0.1043,  0.0192, -0.2053,  0.1305, -0.1481, -0.0378,\n",
      "         -0.0688, -0.0462,  0.2094,  0.0143, -0.0014,  0.0309, -0.0169, -0.0345,\n",
      "          0.2021, -0.1902,  0.1374,  0.0465],\n",
      "        [ 0.1245, -0.2197,  0.1993, -0.1314,  0.1714, -0.0312,  0.1026, -0.1848,\n",
      "          0.1700, -0.0728, -0.0492,  0.0059,  0.0117, -0.1376, -0.0558, -0.0114,\n",
      "         -0.2118, -0.1025,  0.0590, -0.1585],\n",
      "        [-0.1418, -0.0249,  0.0717, -0.0261,  0.0689, -0.1748,  0.1923, -0.1808,\n",
      "         -0.1917,  0.0013, -0.1112,  0.0404,  0.0790,  0.0447, -0.0811,  0.1922,\n",
      "          0.0242,  0.0583,  0.1332, -0.0252],\n",
      "        [-0.0963,  0.0152,  0.0717, -0.2185, -0.1335, -0.1947,  0.0640,  0.1072,\n",
      "         -0.2229,  0.1344, -0.1603, -0.0917,  0.0824,  0.2032,  0.1702,  0.1063,\n",
      "         -0.1360, -0.1757, -0.1731, -0.0696],\n",
      "        [ 0.0725,  0.1704,  0.0997,  0.2043,  0.0688, -0.0117,  0.2096,  0.0783,\n",
      "         -0.1230,  0.1140,  0.0624, -0.0610, -0.1814,  0.0668, -0.1703,  0.1766,\n",
      "         -0.1901,  0.0345, -0.0983, -0.1850],\n",
      "        [ 0.0302,  0.0562, -0.1095,  0.1256, -0.0342,  0.1855, -0.0242,  0.1251,\n",
      "         -0.1449, -0.0959, -0.1742,  0.1644, -0.1222, -0.0955,  0.1298,  0.0068,\n",
      "         -0.0251, -0.0883, -0.2145,  0.1097],\n",
      "        [ 0.1555,  0.1362, -0.0914,  0.1109, -0.1410, -0.0936,  0.0196,  0.0359,\n",
      "          0.0118, -0.1409, -0.1403,  0.0881,  0.0418, -0.1662, -0.1233, -0.0140,\n",
      "         -0.0584,  0.1935, -0.0621, -0.0279],\n",
      "        [ 0.0290,  0.1101,  0.0538, -0.2192, -0.1738,  0.0216, -0.0319, -0.1516,\n",
      "         -0.1606,  0.0704, -0.2066, -0.0188, -0.1856, -0.1835, -0.1663, -0.0233,\n",
      "          0.0656, -0.0151, -0.1184, -0.0544],\n",
      "        [ 0.0689,  0.0566, -0.0290, -0.1043, -0.0711,  0.1253, -0.1266, -0.0645,\n",
      "         -0.0862, -0.1289,  0.1696,  0.1049,  0.0991,  0.1947,  0.1276,  0.1089,\n",
      "         -0.0612, -0.0598,  0.1659, -0.0836]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0173,  0.0105, -0.0735,  0.1950, -0.1483,  0.1686, -0.0195,  0.2158,\n",
      "        -0.1667,  0.2072, -0.0063, -0.0990, -0.1282, -0.1682, -0.0881, -0.1787,\n",
      "        -0.1686,  0.1161, -0.1814,  0.0240], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0998, -0.2192, -0.0061, -0.1398,  0.0227, -0.0419, -0.1111,  0.0821,\n",
      "         -0.1435,  0.1420,  0.1066, -0.0029,  0.0819, -0.2200, -0.1489,  0.1339,\n",
      "         -0.2157, -0.1343, -0.2232, -0.0090]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1944], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for parameter in model.parameters():\n",
    "\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7667],\n",
       "        [-0.7667],\n",
       "        [-0.7667],\n",
       "        ...,\n",
       "        [-0.7667],\n",
       "        [-0.7667],\n",
       "        [-0.8514]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(f_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now define loss based on a PDE. We need to get derivative of p against x and t.\n",
    "MSE_loss = nn.MSELoss()\n",
    "f_sample = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "pt_f_sample = Variable(torch.from_numpy(f_sample).float(), requires_grad=True).to(device)\n",
    "t_sample = np.random.uniform(low=0.0, high=10000, size=(500,1))\n",
    "pt_t_sample = Variable(torch.from_numpy(t_sample).float(), requires_grad=True).to(device)\n",
    "loss_pde = MSE_loss(model.PDE(pt_f_sample, pt_t_sample), torch.zeros_like(model.PDE(pt_f_sample, pt_t_sample)))\n",
    "# In this case, we have p_train to compare directly against estimate of p at f_train, t_train. \n",
    "loss_data = MSE_loss(model(f_train, t_train), p_train)\n",
    "loss = loss_pde + loss_data\n",
    "\n",
    "# In a different scenario, I will put in all f, t data without estimating p before training. \n",
    "# reorganize f and t series so that I have f at each time point (t_i), add - 1/n_i sum(log(p(f(t_i), t_i))) + int model(f, t_i)df\n",
    "# sort the data by time point, start from t=0 till maximum t (either t_extinct or t_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 training loss: tensor(1.2201) s: Parameter containing:\n",
      "tensor(0.0990, requires_grad=True) N: Parameter containing:\n",
      "tensor(499.9998, requires_grad=True)\n",
      "1 training loss: tensor(1.1475) s: Parameter containing:\n",
      "tensor(0.0980, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0001, requires_grad=True)\n",
      "2 training loss: tensor(1.0774) s: Parameter containing:\n",
      "tensor(0.0970, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0002, requires_grad=True)\n",
      "3 training loss: tensor(1.0101) s: Parameter containing:\n",
      "tensor(0.0960, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0004, requires_grad=True)\n",
      "4 training loss: tensor(0.9452) s: Parameter containing:\n",
      "tensor(0.0951, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0006, requires_grad=True)\n",
      "5 training loss: tensor(0.8828) s: Parameter containing:\n",
      "tensor(0.0941, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0009, requires_grad=True)\n",
      "6 training loss: tensor(0.8232) s: Parameter containing:\n",
      "tensor(0.0932, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0012, requires_grad=True)\n",
      "7 training loss: tensor(0.7659) s: Parameter containing:\n",
      "tensor(0.0923, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0015, requires_grad=True)\n",
      "8 training loss: tensor(0.7111) s: Parameter containing:\n",
      "tensor(0.0914, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0018, requires_grad=True)\n",
      "9 training loss: tensor(0.6589) s: Parameter containing:\n",
      "tensor(0.0906, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0021, requires_grad=True)\n",
      "10 training loss: tensor(0.6089) s: Parameter containing:\n",
      "tensor(0.0898, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0024, requires_grad=True)\n",
      "11 training loss: tensor(0.5615) s: Parameter containing:\n",
      "tensor(0.0890, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0027, requires_grad=True)\n",
      "12 training loss: tensor(0.5164) s: Parameter containing:\n",
      "tensor(0.0882, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0030, requires_grad=True)\n",
      "13 training loss: tensor(0.4736) s: Parameter containing:\n",
      "tensor(0.0875, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0032, requires_grad=True)\n",
      "14 training loss: tensor(0.4332) s: Parameter containing:\n",
      "tensor(0.0869, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0035, requires_grad=True)\n",
      "15 training loss: tensor(0.3951) s: Parameter containing:\n",
      "tensor(0.0862, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0037, requires_grad=True)\n",
      "16 training loss: tensor(0.3592) s: Parameter containing:\n",
      "tensor(0.0857, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0039, requires_grad=True)\n",
      "17 training loss: tensor(0.3256) s: Parameter containing:\n",
      "tensor(0.0851, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0042, requires_grad=True)\n",
      "18 training loss: tensor(0.2942) s: Parameter containing:\n",
      "tensor(0.0846, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0043, requires_grad=True)\n",
      "19 training loss: tensor(0.2651) s: Parameter containing:\n",
      "tensor(0.0842, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0045, requires_grad=True)\n",
      "20 training loss: tensor(0.2382) s: Parameter containing:\n",
      "tensor(0.0837, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0046, requires_grad=True)\n",
      "21 training loss: tensor(0.2134) s: Parameter containing:\n",
      "tensor(0.0834, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0048, requires_grad=True)\n",
      "22 training loss: tensor(0.1907) s: Parameter containing:\n",
      "tensor(0.0830, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0049, requires_grad=True)\n",
      "23 training loss: tensor(0.1702) s: Parameter containing:\n",
      "tensor(0.0827, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0050, requires_grad=True)\n",
      "24 training loss: tensor(0.1517) s: Parameter containing:\n",
      "tensor(0.0824, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0051, requires_grad=True)\n",
      "25 training loss: tensor(0.1352) s: Parameter containing:\n",
      "tensor(0.0821, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0052, requires_grad=True)\n",
      "26 training loss: tensor(0.1206) s: Parameter containing:\n",
      "tensor(0.0819, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0052, requires_grad=True)\n",
      "27 training loss: tensor(0.1079) s: Parameter containing:\n",
      "tensor(0.0817, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0053, requires_grad=True)\n",
      "28 training loss: tensor(0.0970) s: Parameter containing:\n",
      "tensor(0.0815, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0054, requires_grad=True)\n",
      "29 training loss: tensor(0.0877) s: Parameter containing:\n",
      "tensor(0.0812, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0054, requires_grad=True)\n",
      "30 training loss: tensor(0.0800) s: Parameter containing:\n",
      "tensor(0.0810, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0055, requires_grad=True)\n",
      "31 training loss: tensor(0.0738) s: Parameter containing:\n",
      "tensor(0.0808, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0056, requires_grad=True)\n",
      "32 training loss: tensor(0.0689) s: Parameter containing:\n",
      "tensor(0.0806, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0056, requires_grad=True)\n",
      "33 training loss: tensor(0.0652) s: Parameter containing:\n",
      "tensor(0.0804, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0057, requires_grad=True)\n",
      "34 training loss: tensor(0.0626) s: Parameter containing:\n",
      "tensor(0.0802, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0058, requires_grad=True)\n",
      "35 training loss: tensor(0.0609) s: Parameter containing:\n",
      "tensor(0.0799, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0059, requires_grad=True)\n",
      "36 training loss: tensor(0.0600) s: Parameter containing:\n",
      "tensor(0.0796, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0060, requires_grad=True)\n",
      "37 training loss: tensor(0.0598) s: Parameter containing:\n",
      "tensor(0.0794, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0060, requires_grad=True)\n",
      "38 training loss: tensor(0.0600) s: Parameter containing:\n",
      "tensor(0.0790, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0061, requires_grad=True)\n",
      "39 training loss: tensor(0.0605) s: Parameter containing:\n",
      "tensor(0.0787, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0062, requires_grad=True)\n",
      "40 training loss: tensor(0.0613) s: Parameter containing:\n",
      "tensor(0.0783, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0063, requires_grad=True)\n",
      "41 training loss: tensor(0.0622) s: Parameter containing:\n",
      "tensor(0.0779, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0063, requires_grad=True)\n",
      "42 training loss: tensor(0.0633) s: Parameter containing:\n",
      "tensor(0.0775, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0065, requires_grad=True)\n",
      "43 training loss: tensor(0.0640) s: Parameter containing:\n",
      "tensor(0.0770, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0066, requires_grad=True)\n",
      "44 training loss: tensor(0.0649) s: Parameter containing:\n",
      "tensor(0.0765, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0067, requires_grad=True)\n",
      "45 training loss: tensor(0.0653) s: Parameter containing:\n",
      "tensor(0.0760, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0068, requires_grad=True)\n",
      "46 training loss: tensor(0.0657) s: Parameter containing:\n",
      "tensor(0.0755, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0070, requires_grad=True)\n",
      "47 training loss: tensor(0.0659) s: Parameter containing:\n",
      "tensor(0.0749, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0071, requires_grad=True)\n",
      "48 training loss: tensor(0.0659) s: Parameter containing:\n",
      "tensor(0.0743, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0073, requires_grad=True)\n",
      "49 training loss: tensor(0.0656) s: Parameter containing:\n",
      "tensor(0.0738, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0074, requires_grad=True)\n",
      "50 training loss: tensor(0.0653) s: Parameter containing:\n",
      "tensor(0.0731, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0076, requires_grad=True)\n",
      "51 training loss: tensor(0.0647) s: Parameter containing:\n",
      "tensor(0.0725, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0077, requires_grad=True)\n",
      "52 training loss: tensor(0.0641) s: Parameter containing:\n",
      "tensor(0.0719, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0079, requires_grad=True)\n",
      "53 training loss: tensor(0.0634) s: Parameter containing:\n",
      "tensor(0.0712, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0081, requires_grad=True)\n",
      "54 training loss: tensor(0.0626) s: Parameter containing:\n",
      "tensor(0.0706, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0082, requires_grad=True)\n",
      "55 training loss: tensor(0.0618) s: Parameter containing:\n",
      "tensor(0.0699, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0084, requires_grad=True)\n",
      "56 training loss: tensor(0.0610) s: Parameter containing:\n",
      "tensor(0.0693, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0086, requires_grad=True)\n",
      "57 training loss: tensor(0.0603) s: Parameter containing:\n",
      "tensor(0.0686, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0088, requires_grad=True)\n",
      "58 training loss: tensor(0.0596) s: Parameter containing:\n",
      "tensor(0.0679, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0090, requires_grad=True)\n",
      "59 training loss: tensor(0.0590) s: Parameter containing:\n",
      "tensor(0.0673, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0092, requires_grad=True)\n",
      "60 training loss: tensor(0.0583) s: Parameter containing:\n",
      "tensor(0.0666, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0094, requires_grad=True)\n",
      "61 training loss: tensor(0.0578) s: Parameter containing:\n",
      "tensor(0.0660, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0096, requires_grad=True)\n",
      "62 training loss: tensor(0.0573) s: Parameter containing:\n",
      "tensor(0.0654, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0098, requires_grad=True)\n",
      "63 training loss: tensor(0.0570) s: Parameter containing:\n",
      "tensor(0.0648, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0100, requires_grad=True)\n",
      "64 training loss: tensor(0.0567) s: Parameter containing:\n",
      "tensor(0.0642, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0102, requires_grad=True)\n",
      "65 training loss: tensor(0.0564) s: Parameter containing:\n",
      "tensor(0.0636, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0104, requires_grad=True)\n",
      "66 training loss: tensor(0.0562) s: Parameter containing:\n",
      "tensor(0.0630, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0106, requires_grad=True)\n",
      "67 training loss: tensor(0.0560) s: Parameter containing:\n",
      "tensor(0.0624, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0107, requires_grad=True)\n",
      "68 training loss: tensor(0.0559) s: Parameter containing:\n",
      "tensor(0.0618, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0110, requires_grad=True)\n",
      "69 training loss: tensor(0.0558) s: Parameter containing:\n",
      "tensor(0.0613, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0112, requires_grad=True)\n",
      "70 training loss: tensor(0.0558) s: Parameter containing:\n",
      "tensor(0.0607, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0114, requires_grad=True)\n",
      "71 training loss: tensor(0.0557) s: Parameter containing:\n",
      "tensor(0.0602, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0116, requires_grad=True)\n",
      "72 training loss: tensor(0.0556) s: Parameter containing:\n",
      "tensor(0.0597, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0118, requires_grad=True)\n",
      "73 training loss: tensor(0.0556) s: Parameter containing:\n",
      "tensor(0.0592, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0120, requires_grad=True)\n",
      "74 training loss: tensor(0.0555) s: Parameter containing:\n",
      "tensor(0.0587, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0122, requires_grad=True)\n",
      "75 training loss: tensor(0.0554) s: Parameter containing:\n",
      "tensor(0.0582, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0124, requires_grad=True)\n",
      "76 training loss: tensor(0.0555) s: Parameter containing:\n",
      "tensor(0.0577, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0125, requires_grad=True)\n",
      "77 training loss: tensor(0.0552) s: Parameter containing:\n",
      "tensor(0.0572, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0127, requires_grad=True)\n",
      "78 training loss: tensor(0.0550) s: Parameter containing:\n",
      "tensor(0.0568, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0128, requires_grad=True)\n",
      "79 training loss: tensor(0.0548) s: Parameter containing:\n",
      "tensor(0.0563, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0130, requires_grad=True)\n",
      "80 training loss: tensor(0.0547) s: Parameter containing:\n",
      "tensor(0.0558, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0132, requires_grad=True)\n",
      "81 training loss: tensor(0.0547) s: Parameter containing:\n",
      "tensor(0.0554, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0133, requires_grad=True)\n",
      "82 training loss: tensor(0.0546) s: Parameter containing:\n",
      "tensor(0.0549, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0135, requires_grad=True)\n",
      "83 training loss: tensor(0.0544) s: Parameter containing:\n",
      "tensor(0.0545, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0136, requires_grad=True)\n",
      "84 training loss: tensor(0.0543) s: Parameter containing:\n",
      "tensor(0.0541, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0137, requires_grad=True)\n",
      "85 training loss: tensor(0.0541) s: Parameter containing:\n",
      "tensor(0.0537, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0139, requires_grad=True)\n",
      "86 training loss: tensor(0.0541) s: Parameter containing:\n",
      "tensor(0.0532, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0140, requires_grad=True)\n",
      "87 training loss: tensor(0.0540) s: Parameter containing:\n",
      "tensor(0.0528, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0142, requires_grad=True)\n",
      "88 training loss: tensor(0.0539) s: Parameter containing:\n",
      "tensor(0.0524, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0143, requires_grad=True)\n",
      "89 training loss: tensor(0.0538) s: Parameter containing:\n",
      "tensor(0.0520, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0145, requires_grad=True)\n",
      "90 training loss: tensor(0.0537) s: Parameter containing:\n",
      "tensor(0.0516, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0146, requires_grad=True)\n",
      "91 training loss: tensor(0.0536) s: Parameter containing:\n",
      "tensor(0.0512, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0148, requires_grad=True)\n",
      "92 training loss: tensor(0.0534) s: Parameter containing:\n",
      "tensor(0.0507, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0150, requires_grad=True)\n",
      "93 training loss: tensor(0.0533) s: Parameter containing:\n",
      "tensor(0.0503, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0152, requires_grad=True)\n",
      "94 training loss: tensor(0.0533) s: Parameter containing:\n",
      "tensor(0.0499, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0154, requires_grad=True)\n",
      "95 training loss: tensor(0.0533) s: Parameter containing:\n",
      "tensor(0.0495, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0155, requires_grad=True)\n",
      "96 training loss: tensor(0.0532) s: Parameter containing:\n",
      "tensor(0.0491, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0157, requires_grad=True)\n",
      "97 training loss: tensor(0.0531) s: Parameter containing:\n",
      "tensor(0.0487, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0158, requires_grad=True)\n",
      "98 training loss: tensor(0.0530) s: Parameter containing:\n",
      "tensor(0.0483, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0160, requires_grad=True)\n",
      "99 training loss: tensor(0.0530) s: Parameter containing:\n",
      "tensor(0.0478, requires_grad=True) N: Parameter containing:\n",
      "tensor(500.0162, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "    MSE_loss = nn.MSELoss()\n",
    "    f_sample = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "    pt_f_sample = Variable(torch.from_numpy(f_sample).float(), requires_grad=True).to(device)\n",
    "    t_sample = np.random.uniform(low=0.0, high=10000, size=(500,1))\n",
    "    pt_t_sample = Variable(torch.from_numpy(t_sample).float(), requires_grad=True).to(device)\n",
    "    loss_pde = MSE_loss(model.PDE(pt_f_sample, pt_t_sample), torch.zeros_like(model.PDE(pt_f_sample, pt_t_sample)))\n",
    "    # In this case, we have p_train to compare directly against estimate of p at f_train, t_train. \n",
    "    loss_data = MSE_loss(model(f_train, t_train), p_train)\n",
    "    loss = loss_pde + loss_data\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(epoch,\"training loss:\", loss.data, \"s:\", model.s, \"N:\", model.N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s and N are not updating... why??? ==> solved! self.s and self.N should be torch.nn.Parameter, not Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367],\n",
       "        [ 0.3479],\n",
       "        [ 0.3423],\n",
       "        ...,\n",
       "        [ 0.3477],\n",
       "        [ 0.3455],\n",
       "        [-0.1735]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(f_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0723],\n",
       "        [0.0780],\n",
       "        [0.3574],\n",
       "        ...,\n",
       "        [0.6743],\n",
       "        [0.5192],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(500.0162, requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.0478, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return predicted s and N, plot p(f, t). Plot p(f = 1, t), which is fixation probability as a function of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
