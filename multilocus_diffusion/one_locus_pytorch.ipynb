{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n",
      "[4254 8210 7519 ...  969 8497  219]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_train = 5000\n",
    "# Load Data # replace with something else (also will make .pkl file)\n",
    "data = np.load('single_locus_data.npy', allow_pickle='TRUE')\n",
    "# with open(\"single_locus_data.pkl\", \"wb\") as tf:\n",
    "#     data = pickle.load(tf)    \n",
    "f = data.item()['f'][:, None]\n",
    "t = data.item()['t'][:, None]\n",
    "p = data.item()['phi'][:, None]\n",
    "print(len(f))\n",
    "idx_list = np.arange(len(f))\n",
    "np.random.shuffle(idx_list)\n",
    "print(idx_list)\n",
    "\n",
    "f_train = torch.from_numpy(f[idx_list[:N_train]]).float()\n",
    "t_train = torch.from_numpy(t[idx_list[:N_train]]).float()\n",
    "p_train = torch.from_numpy(p[idx_list[:N_train]]).float()\n",
    "f_test = torch.from_numpy(f[idx_list[N_train:]]).float()\n",
    "t_test = torch.from_numpy(t[idx_list[N_train:]]).float()\n",
    "p_test = torch.from_numpy(p[idx_list[N_train:]]).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers, s0, N0):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(layers)-3):\n",
    "            self.model.add_module(f\"layer{i}\", nn.Linear(layers[i], layers[i+1]))\n",
    "            self.model.add_module(f\"activation{i}\", nn.Tanh())\n",
    "        self.model.add_module(f\"layer{len(layers)-2}\", nn.Linear(layers[-3], layers[-2]))\n",
    "        self.model.add_module(f\"activation{len(layers)-2}\", nn.Softplus())\n",
    "        self.model.add_module(f\"layer{len(layers)-1}\", nn.Linear(layers[-2], layers[-1]))\n",
    "        self.s = Variable(torch.tensor(s0).float(), requires_grad = True).to(device)\n",
    "        self.N = Variable(torch.tensor(N0).float(), requires_grad = True).to(device)\n",
    "\n",
    "    def forward(self, f, t):\n",
    "        inputs = torch.cat([f, t], 1)\n",
    "        return self.model(inputs)\n",
    "    def PDE(self, f, t):\n",
    "        s = self.s\n",
    "        N = self.N\n",
    "        p = self.model(torch.cat([f, t], 1))\n",
    "        p_f = torch.autograd.grad(p.sum(), f, create_graph=True)[0]\n",
    "        p_t = torch.autograd.grad(p.sum(), t, create_graph=True)[0]\n",
    "        p_ff = torch.autograd.grad(p_f.sum(), f, create_graph=True)[0]\n",
    "        g = p_t + s * ((1 - 2*f)*p + (f - f**2)*p_f) + 1 / (2*N) * (-2*p + 2*(1 - 2*f)*p_f + (f - f**2)*p_ff)\n",
    "        return g\n",
    "\n",
    "# Input = (f, t) series, Output = p series\n",
    "model = PINN(layers = [2, 20, 20, 20, 1], s0 = 0.1, N0 = 500).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN(\n",
      "  (model): Sequential(\n",
      "    (layer0): Linear(in_features=2, out_features=20, bias=True)\n",
      "    (activation0): Tanh()\n",
      "    (layer1): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (activation1): Tanh()\n",
      "    (layer3): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (activation3): Softplus(beta=1, threshold=20)\n",
      "    (layer4): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2719],\n",
       "        [0.2718],\n",
       "        [0.2718],\n",
       "        ...,\n",
       "        [0.2831],\n",
       "        [0.2723],\n",
       "        [0.2756]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(f_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now define loss based on a PDE. We need to get derivative of p against x and t.\n",
    "MSE_loss = nn.MSELoss()\n",
    "f_sample = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "pt_f_sample = Variable(torch.from_numpy(f_sample).float(), requires_grad=True).to(device)\n",
    "t_sample = np.random.uniform(low=0.0, high=10000, size=(500,1))\n",
    "pt_t_sample = Variable(torch.from_numpy(t_sample).float(), requires_grad=True).to(device)\n",
    "loss_pde = MSE_loss(model.PDE(pt_f_sample, pt_t_sample), torch.zeros_like(model.PDE(pt_f_sample, pt_t_sample)))\n",
    "# In this case, we have p_train to compare directly against estimate of p at f_train, t_train. \n",
    "loss_data = MSE_loss(model(f_train, t_train), p_train)\n",
    "loss = loss_pde + loss_data\n",
    "\n",
    "# In a different scenario, I will put in all f, t data without estimating p before training. \n",
    "# reorganize f and t series so that I have f at each time point (t_i), add - 1/n_i sum(log(p(f(t_i), t_i))) + int model(f, t_i)df\n",
    "# sort the data by time point, start from t=0 till maximum t (either t_extinct or t_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 training loss: tensor(0.0691) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "1 training loss: tensor(0.0669) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "2 training loss: tensor(0.0661) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "3 training loss: tensor(0.0652) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "4 training loss: tensor(0.0641) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "5 training loss: tensor(0.0629) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "6 training loss: tensor(0.0617) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "7 training loss: tensor(0.0607) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "8 training loss: tensor(0.0598) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "9 training loss: tensor(0.0589) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "10 training loss: tensor(0.0582) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "11 training loss: tensor(0.0574) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "12 training loss: tensor(0.0566) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "13 training loss: tensor(0.0559) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "14 training loss: tensor(0.0553) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "15 training loss: tensor(0.0547) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "16 training loss: tensor(0.0543) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "17 training loss: tensor(0.0539) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "18 training loss: tensor(0.0535) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "19 training loss: tensor(0.0532) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "20 training loss: tensor(0.0528) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "21 training loss: tensor(0.0526) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "22 training loss: tensor(0.0523) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "23 training loss: tensor(0.0521) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "24 training loss: tensor(0.0520) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "25 training loss: tensor(0.0519) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "26 training loss: tensor(0.0518) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "27 training loss: tensor(0.0518) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "28 training loss: tensor(0.0518) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "29 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "30 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "31 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "32 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "33 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "34 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "35 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "36 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "37 training loss: tensor(0.0518) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "38 training loss: tensor(0.0518) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "39 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "40 training loss: tensor(0.0517) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "41 training loss: tensor(0.0516) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "42 training loss: tensor(0.0515) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "43 training loss: tensor(0.0515) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "44 training loss: tensor(0.0513) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "45 training loss: tensor(0.0512) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "46 training loss: tensor(0.0511) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "47 training loss: tensor(0.0508) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "48 training loss: tensor(0.0506) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "49 training loss: tensor(0.0501) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "50 training loss: tensor(0.0497) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "51 training loss: tensor(0.0492) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "52 training loss: tensor(0.0489) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "53 training loss: tensor(0.0492) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "54 training loss: tensor(0.0489) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "55 training loss: tensor(0.0485) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "56 training loss: tensor(0.0482) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "57 training loss: tensor(0.0479) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "58 training loss: tensor(0.0476) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "59 training loss: tensor(0.0473) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "60 training loss: tensor(0.0471) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "61 training loss: tensor(0.0469) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "62 training loss: tensor(0.0466) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "63 training loss: tensor(0.0461) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "64 training loss: tensor(0.0457) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "65 training loss: tensor(0.0453) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "66 training loss: tensor(0.0449) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "67 training loss: tensor(0.0445) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "68 training loss: tensor(0.0441) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "69 training loss: tensor(0.0436) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "70 training loss: tensor(0.0430) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "71 training loss: tensor(0.0423) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "72 training loss: tensor(0.0418) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "73 training loss: tensor(0.0416) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "74 training loss: tensor(0.0416) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "75 training loss: tensor(0.0414) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "76 training loss: tensor(0.0412) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "77 training loss: tensor(0.0408) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "78 training loss: tensor(0.0404) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "79 training loss: tensor(0.0400) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "80 training loss: tensor(0.0399) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "81 training loss: tensor(0.0396) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "82 training loss: tensor(0.0393) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "83 training loss: tensor(0.0392) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "84 training loss: tensor(0.0390) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "85 training loss: tensor(0.0388) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "86 training loss: tensor(0.0385) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "87 training loss: tensor(0.0383) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "88 training loss: tensor(0.0381) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "89 training loss: tensor(0.0378) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "90 training loss: tensor(0.0376) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "91 training loss: tensor(0.0375) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "92 training loss: tensor(0.0373) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "93 training loss: tensor(0.0371) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "94 training loss: tensor(0.0369) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "95 training loss: tensor(0.0367) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "96 training loss: tensor(0.0365) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "97 training loss: tensor(0.0364) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "98 training loss: tensor(0.0364) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n",
      "99 training loss: tensor(0.0364) s: tensor(0.1000, requires_grad=True) N: tensor(500., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "    MSE_loss = nn.MSELoss()\n",
    "    f_sample = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n",
    "    pt_f_sample = Variable(torch.from_numpy(f_sample).float(), requires_grad=True).to(device)\n",
    "    t_sample = np.random.uniform(low=0.0, high=10000, size=(500,1))\n",
    "    pt_t_sample = Variable(torch.from_numpy(t_sample).float(), requires_grad=True).to(device)\n",
    "    loss_pde = MSE_loss(model.PDE(pt_f_sample, pt_t_sample), torch.zeros_like(model.PDE(pt_f_sample, pt_t_sample)))\n",
    "    # In this case, we have p_train to compare directly against estimate of p at f_train, t_train. \n",
    "    loss_data = MSE_loss(model(f_train, t_train), p_train)\n",
    "    loss = loss_pde + loss_data\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        print(epoch,\"training loss:\", loss.data, \"s:\", model.s, \"N:\", model.N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s and N are not updating... why???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3398],\n",
       "        [-0.0076],\n",
       "        [ 0.3392],\n",
       "        ...,\n",
       "        [ 0.3375],\n",
       "        [ 0.3398],\n",
       "        [ 0.3325]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(f_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7249],\n",
       "        [0.0000],\n",
       "        [0.6302],\n",
       "        ...,\n",
       "        [0.5322],\n",
       "        [0.1836],\n",
       "        [0.3461]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return predicted s and N, plot p(f, t). Plot p(f = 1, t), which is fixation probability as a function of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
